{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 4 CSV files of the data records and each CSV file contains attack and normal records.\n",
    "<table>\n",
    "<tr>\n",
    "<th> file name </th>\n",
    "<th> file name size</th>\n",
    "<th> number of records </th>\n",
    "<th> number of features </th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> UNSWNB15_1.csv </td>\n",
    "<td> 165.02 MB </td>\n",
    "<td> 700000 </td>\n",
    "<td> 49 </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> UNSWNB15_2.csv </td>\n",
    "<td> 161.349 MB </td>\n",
    "<td> 700000 </td>\n",
    "<td> 49 </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> UNSWNB15_3.csv </td>\n",
    "<td> 150.965 MB </td>\n",
    "<td> 700000 </td>\n",
    "<td> 49 </td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td> UNSWNB15_4.csv </td>\n",
    "<td> 95.302 MB </td>\n",
    "<td> 440044 </td>\n",
    "<td> 49 </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 49 features.\n",
    "<br>\n",
    "There are 3 different datatypes:\n",
    "- Categorical: proto, state, service, attack_cat\n",
    "- Binary: is_sm_ips_ports, is_ftp_login\n",
    "- Numerical: Rest of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT MODUL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Problem Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Binary classification of attack category*\n",
    "\n",
    "The dataset has \"label\" with 0 and 1 where 0 represents non-attack and 1 represent attack. So with the features available we will try to predict a given datapoint whether it belongs to attack or non-attack category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSW-NB15: Data cleaning and preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for csv files and dataframe\n",
    "import pickle  # To load data int disk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty dict, where I will save all parameters required for test data transformation\n",
    "\n",
    "saved_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading datasets\n",
    "dfs = []\n",
    "for i in range(1,5):\n",
    "    path = 'Dataset/UNSW-NB15_{}.csv'  # There are 4 input csv files\n",
    "    dfs.append(pd.read_csv(path.format(i), header = None))\n",
    "df = pd.concat(dfs).reset_index(drop=True)  # Concat all to a single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from NUSW-NB15_features.csv\n",
    "df_features = pd.read_csv('Dataset/NUSW-NB15_features.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Apply features to the dataset\n",
    "df.columns = df_features['Name'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making column names lower case, removing spaces\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2540047, 49)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srcip</th>\n",
       "      <th>sport</th>\n",
       "      <th>dstip</th>\n",
       "      <th>dsport</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src__ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.166.0.0</td>\n",
       "      <td>1390</td>\n",
       "      <td>149.171.126.6</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.166.0.0</td>\n",
       "      <td>33661</td>\n",
       "      <td>149.171.126.9</td>\n",
       "      <td>1024</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>528</td>\n",
       "      <td>304</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.166.0.6</td>\n",
       "      <td>1464</td>\n",
       "      <td>149.171.126.7</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.166.0.5</td>\n",
       "      <td>3593</td>\n",
       "      <td>149.171.126.5</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>132</td>\n",
       "      <td>164</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.166.0.3</td>\n",
       "      <td>49664</td>\n",
       "      <td>149.171.126.0</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        srcip  sport          dstip dsport proto state       dur  sbytes  \\\n",
       "0  59.166.0.0   1390  149.171.126.6     53   udp   CON  0.001055     132   \n",
       "1  59.166.0.0  33661  149.171.126.9   1024   udp   CON  0.036133     528   \n",
       "2  59.166.0.6   1464  149.171.126.7     53   udp   CON  0.001119     146   \n",
       "3  59.166.0.5   3593  149.171.126.5     53   udp   CON  0.001209     132   \n",
       "4  59.166.0.3  49664  149.171.126.0     53   udp   CON  0.001169     146   \n",
       "\n",
       "   dbytes  sttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst ct_dst_ltm  \\\n",
       "0     164    31  ...           0           3           7          1   \n",
       "1     304    31  ...           0           2           4          2   \n",
       "2     178    31  ...           0          12           8          1   \n",
       "3     164    31  ...           0           6           9          1   \n",
       "4     178    31  ...           0           7           9          1   \n",
       "\n",
       "   ct_src__ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
       "0            3                 1                 1               1   \n",
       "1            3                 1                 1               2   \n",
       "2            2                 2                 1               1   \n",
       "3            1                 1                 1               1   \n",
       "4            1                 1                 1               1   \n",
       "\n",
       "   attack_cat  label  \n",
       "0         NaN      0  \n",
       "1         NaN      0  \n",
       "2         NaN      0  \n",
       "3         NaN      0  \n",
       "4         NaN      0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> pre-processing & Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (X_train)\n",
    "# print (y_train)\n",
    "# print (X_test)\n",
    "# print (y_test)\n",
    "# Print Null values in X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null values in X_train before filter: 4497854\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Null values in X_train before filter:\", X_train.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srcip                     0\n",
      "sport                     0\n",
      "dstip                     0\n",
      "dsport                    0\n",
      "proto                     0\n",
      "state                     0\n",
      "dur                       0\n",
      "sbytes                    0\n",
      "dbytes                    0\n",
      "sttl                      0\n",
      "dttl                      0\n",
      "sloss                     0\n",
      "dloss                     0\n",
      "service                   0\n",
      "sload                     0\n",
      "dload                     0\n",
      "spkts                     0\n",
      "dpkts                     0\n",
      "swin                      0\n",
      "dwin                      0\n",
      "stcpb                     0\n",
      "dtcpb                     0\n",
      "smeansz                   0\n",
      "dmeansz                   0\n",
      "trans_depth               0\n",
      "res_bdy_len               0\n",
      "sjit                      0\n",
      "djit                      0\n",
      "stime                     0\n",
      "ltime                     0\n",
      "sintpkt                   0\n",
      "dintpkt                   0\n",
      "tcprtt                    0\n",
      "synack                    0\n",
      "ackdat                    0\n",
      "is_sm_ips_ports           0\n",
      "ct_state_ttl              0\n",
      "ct_flw_http_mthd    1213552\n",
      "is_ftp_login        1287353\n",
      "ct_ftp_cmd                0\n",
      "ct_srv_src                0\n",
      "ct_srv_dst                0\n",
      "ct_dst_ltm                0\n",
      "ct_src__ltm               0\n",
      "ct_src_dport_ltm          0\n",
      "ct_dst_sport_ltm          0\n",
      "ct_dst_src_ltm            0\n",
      "attack_cat          1996949\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print Null values in X_train\n",
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "Generic             193696\n",
       "Exploits             40112\n",
       " Fuzzers             17358\n",
       "DoS                  14723\n",
       " Reconnaissance      11025\n",
       " Fuzzers              4562\n",
       "Analysis              2400\n",
       "Backdoor              1617\n",
       "Reconnaissance        1599\n",
       " Shellcode            1167\n",
       "Backdoors              486\n",
       "Shellcode              192\n",
       "Worms                  156\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['attack_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in attack_cat column with 'normal'\n",
    "X_train['attack_cat'] = X_train['attack_cat'].fillna('normal')\n",
    "X_test['attack_cat'] = X_test['attack_cat'].fillna('normal')\n",
    "\n",
    "# Fill null values in ct_flw_http_mthd column with 0\n",
    "X_train['ct_flw_http_mthd'] = X_train['ct_flw_http_mthd'].fillna(0)\n",
    "X_test['ct_flw_http_mthd'] = X_test['ct_flw_http_mthd'].fillna(0)\n",
    "\n",
    "# Fill null values in is_ftp_login column with 0\n",
    "X_train['is_ftp_login'] = X_train['is_ftp_login'].fillna(0)\n",
    "X_test['is_ftp_login'] = X_test['is_ftp_login'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null values in X_train after filter: 0\n"
     ]
    }
   ],
   "source": [
    "# check null values in X_train\n",
    "print(\"Total Null values in X_train after filter:\", X_train.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape before high corr filter: (2286042, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape before high corr filter:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'service',\n",
      "       'ct_ftp_cmd', 'attack_cat'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "on_numeric_columns = X_train.select_dtypes(exclude='number').columns\n",
    "non_numeric_columns = X_test.select_dtypes(exclude='number').columns\n",
    "print(f\"Non-numeric columns: {non_numeric_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['dsport'] = X_train['dsport'].astype(str)\n",
    "X_train['dsport'].fillna('missing', inplace=True)  # Replace NaN with a placeholder value\n",
    "X_train = X_train.dropna(subset=['dsport'])\n",
    "X_train['ct_ftp_cmd'] = X_train['ct_ftp_cmd'].astype(str)\n",
    "X_train['srcip'] = X_train['srcip'].astype(str)\n",
    "\n",
    "X_test['ct_ftp_cmd'] = X_test['ct_ftp_cmd'].astype(str)\n",
    "X_test['srcip'] = X_test['srcip'].astype(str)\n",
    "X_test['dsport'] = X_test['dsport'].astype(str)\n",
    "X_test['dsport'].fillna('missing', inplace=True)  # Replace NaN with a placeholder value\n",
    "X_test = X_test.dropna(subset=['dsport'])\n",
    "\n",
    "# Assuming 'X_train' is your DataFrame\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "# Now apply the LabelEncoder\n",
    "X_train['ct_ftp_cmd'] = le.fit_transform(X_train['ct_ftp_cmd'])\n",
    "X_test['ct_ftp_cmd'] = le.fit_transform(X_test['ct_ftp_cmd'])\n",
    "X_train['srcip'] = le.fit_transform(X_train['srcip'])\n",
    "X_test['srcip'] = le.fit_transform(X_test['srcip'])\n",
    "# Convert the 'sport' column to strings\n",
    "X_train['sport'] = X_train['sport'].astype(str)\n",
    "X_test['sport'] = X_test['sport'].astype(str)\n",
    "# Now, you can use LabelEncoder on the 'sport' column\n",
    "X_train['sport'] = le.fit_transform(X_train['sport'])\n",
    "X_train['dsport'] = le.fit_transform(X_train['dsport'])\n",
    "X_test['sport'] = le.fit_transform(X_test['sport'])\n",
    "X_test['dsport'] = le.fit_transform(X_test['dsport'])\n",
    "\n",
    "# Repeat for other columns...\n",
    "\n",
    "X_train['dstip'] = le.fit_transform(X_train['dstip'])\n",
    "X_train['dsport'] = le.fit_transform(X_train['dsport'])\n",
    "X_train['proto'] = le.fit_transform(X_train['proto'])\n",
    "X_train['state'] = le.fit_transform(X_train['state'])\n",
    "X_train['service'] = le.fit_transform(X_train['service'])\n",
    "X_train['ct_ftp_cmd'] = le.fit_transform(X_train['ct_ftp_cmd'])\n",
    "X_train['attack_cat'] = le.fit_transform(X_train['attack_cat'])\n",
    "X_train['dstip'] = le.fit_transform(X_train['dstip'])\n",
    "\n",
    "X_test['dstip'] = le.fit_transform(X_test['dstip'])\n",
    "X_test['dsport'] = le.fit_transform(X_test['dsport'])\n",
    "X_test['proto'] = le.fit_transform(X_test['proto'])\n",
    "X_test['state'] = le.fit_transform(X_test['state'])\n",
    "X_test['service'] = le.fit_transform(X_test['service'])\n",
    "X_test['ct_ftp_cmd'] = le.fit_transform(X_test['ct_ftp_cmd'])\n",
    "X_test['attack_cat'] = le.fit_transform(X_test['attack_cat'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index([], dtype='object')\n",
      "Non-numeric columns: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "non_numeric_columns = X_train.select_dtypes(exclude='number').columns\n",
    "print(f\"Non-numeric columns: {non_numeric_columns}\")\n",
    "non_numeric_columns = X_test.select_dtypes(exclude='number').columns\n",
    "print(f\"Non-numeric columns: {non_numeric_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High corr: sbytes               sloss                0.9544266892460995\n",
      "High corr: dbytes               dloss                0.9913052585818809\n",
      "High corr: dbytes               dpkts                0.9706287326945696\n",
      "High corr: sttl                 ct_state_ttl         0.9059574874443663\n",
      "High corr: dloss                dpkts                0.9920944071571861\n",
      "High corr: swin                 dwin                 0.9972098797520872\n",
      "High corr: stime                ltime                0.9999999997825213\n",
      "High corr: tcprtt               synack               0.9297052338631611\n",
      "High corr: tcprtt               ackdat               0.9186177806539728\n",
      "High corr: ct_srv_src           ct_srv_dst           0.9567382933002961\n",
      "High corr: ct_srv_src           ct_dst_src_ltm       0.942191554375676\n",
      "High corr: ct_srv_dst           ct_dst_src_ltm       0.9509948411308352\n",
      "High corr: ct_dst_ltm           ct_src__ltm          0.9385080233831757\n",
      "High corr: ct_dst_ltm           ct_src_dport_ltm     0.9601365848084313\n",
      "High corr: ct_src__ltm          ct_src_dport_ltm     0.9453129990182536\n",
      "High corr: ct_src_dport_ltm     ct_dst_sport_ltm     0.9214794684040317\n",
      "High corr: ct_src_dport_ltm     ct_dst_src_ltm       0.9109669658376368\n"
     ]
    }
   ],
   "source": [
    "# Finding dan Remove high correlation features\n",
    "corr_mat = X_train.corr(method='pearson')\n",
    "columns = corr_mat.columns\n",
    "for i in range(corr_mat.shape[0]):\n",
    "    for j in range(i+1, corr_mat.shape[0]):\n",
    "        if corr_mat.iloc[i, j] >= 0.9:\n",
    "            print(f\"High corr: {columns[i]:20s} {columns[j]:20s} {corr_mat.iloc[i, j]}\")\n",
    "\n",
    "            # Dropping high correlation features\n",
    "            if columns[j] in X_train.columns:\n",
    "                X_train = X_train.drop(columns=[columns[j]])\n",
    "            \n",
    "            if columns[j] in X_test.columns:\n",
    "                X_test = X_test.drop(columns=[columns[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape after high corr filter: (2286042, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape after high corr filter:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after scaling:\n",
      "         srcip  sport  dstip  dsport  proto  state       dur  sbytes  dbytes  \\\n",
      "282001      37  34830      9   47340    120      2  0.001097     146     178   \n",
      "336847      36  61619     26   26636    114      5  0.004232     528    8824   \n",
      "1362417     28  18499     10    3845    114      5  0.777348     588     354   \n",
      "218834      35  19412     24   48764    120      2  0.001681     528     304   \n",
      "1203820     12    453     28   47340    120      6  0.000010     264       0   \n",
      "...        ...    ...    ...     ...    ...    ...       ...     ...     ...   \n",
      "2249467     14    453     29   47340    120      6  0.000009     264       0   \n",
      "963395      34  25616      8   46128    114      5  0.426193    2054    2478   \n",
      "2215104     31  48738     19   37974    114      5  0.667889    1058     766   \n",
      "1484405     41   6632      8   14868    114      5  0.041776    3302   37162   \n",
      "305711      37  31728     21   32567    114      5  0.028309    4014   57706   \n",
      "\n",
      "         sttl  ...    sintpkt    dintpkt    tcprtt  is_sm_ips_ports  \\\n",
      "282001     31  ...   0.006000   0.003000  0.000000                0   \n",
      "336847     31  ...   0.434778   0.337364  0.000672                0   \n",
      "1362417   254  ...  86.358667  99.440000  0.157336                0   \n",
      "218834     31  ...   0.438333   0.210667  0.000000                0   \n",
      "1203820    60  ...   0.010000   0.000000  0.000000                0   \n",
      "...       ...  ...        ...        ...       ...              ...   \n",
      "2249467    60  ...   0.009000   0.000000  0.000000                0   \n",
      "963395     31  ...  20.278047  18.502479  0.000801                0   \n",
      "2215104   254  ...  59.081454  85.865141  0.133565                0   \n",
      "1484405    31  ...   0.813118   0.749618  0.000797                0   \n",
      "305711     31  ...   0.416866   0.407441  0.000724                0   \n",
      "\n",
      "         ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_dst_ltm  \\\n",
      "282001                0.0           0.0           1           3           2   \n",
      "336847                0.0           0.0           1           3           6   \n",
      "1362417               0.0           0.0           0           2           1   \n",
      "218834                0.0           0.0           1           5          10   \n",
      "1203820               0.0           0.0           0          25          16   \n",
      "...                   ...           ...         ...         ...         ...   \n",
      "2249467               0.0           0.0           0          23          23   \n",
      "963395                0.0           0.0           1           6           3   \n",
      "2215104               0.0           0.0           0           2           1   \n",
      "1484405               0.0           0.0           0           6           4   \n",
      "305711                0.0           0.0           1          11           2   \n",
      "\n",
      "         attack_cat  \n",
      "282001           13  \n",
      "336847           13  \n",
      "1362417          13  \n",
      "218834           13  \n",
      "1203820          13  \n",
      "...             ...  \n",
      "2249467          13  \n",
      "963395           13  \n",
      "2215104           1  \n",
      "1484405          13  \n",
      "305711           13  \n",
      "\n",
      "[2286042 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data after scaling:\\n{X_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> TRAINING, Save Model, And Clasification\n",
    "\n",
    "<H5>Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time taken (Random Forest):  93.5866277217865\n",
      "Random Forest model saved as random_forest_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest classifier\n",
    "start_time = time.time()\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "random_forest_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Training time taken (Random Forest): \", end_time - start_time)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "model_filename = 'random_forest_model.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(random_forest_clf, model_file)\n",
    "print(f\"Random Forest model saved as {model_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9981220842109407\n",
      "Precision: 0.9981495053341348\n",
      "Recall: 0.9981220842109407\n",
      "Confusion Matrix:\n",
      "[[221338    477]\n",
      " [     0  32190]]\n"
     ]
    }
   ],
   "source": [
    "# Test and get accuracy, precision, recall\n",
    "y_pred = random_forest_clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time taken (Decision Tree):  3.848851203918457\n",
      "Decision Tree model saved as decision_tree_model.pkl\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "Confusion Matrix:\n",
      "[[221815      0]\n",
      " [     0  32190]]\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree classifier\n",
    "start_time = time.time()\n",
    "decision_tree_clf = DecisionTreeClassifier()\n",
    "decision_tree_clf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(\"Training time taken (Decision Tree): \", end_time - start_time)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "model_filename = 'decision_tree_model.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(decision_tree_clf, model_file)\n",
    "print(f\"Decision Tree model saved as {model_filename}\")\n",
    "\n",
    "# Test and get accuracy, precision, recall\n",
    "y_pred = decision_tree_clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time taken (Neural Network):  140.66628170013428\n",
      "Neural Network model saved as neural_network_model.pkl\n",
      "Test Accuracy: 0.8617310682860574\n",
      "Test Precision: 0.8990055654718714\n",
      "Test Recall: 0.8617310682860574\n",
      "Confusion Matrix:\n",
      "[[194583  27232]\n",
      " [  7889  24301]]\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Create a neural network classifier\n",
    "neural_network_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=0)\n",
    "\n",
    "# Measure the training time\n",
    "start_time = time.time()\n",
    "neural_network_clf.fit(X_train, y_train_encoded)\n",
    "end_time = time.time()\n",
    "print(\"Training time taken (Neural Network): \", end_time - start_time)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "model_filename = 'neural_network_model.pkl'\n",
    "with open(model_filename, 'wb') as model_file:\n",
    "    pickle.dump(neural_network_clf, model_file)\n",
    "print(f\"Neural Network model saved as {model_filename}\")\n",
    "\n",
    "# Test and get accuracy, precision, recall on the test set\n",
    "y_pred_encoded = neural_network_clf.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Making the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>LOAD MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('random_forest_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "# Now, you can use the loaded_model for predictions\n",
    "loaded_y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "with open('decision_tree_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "# Now, you can use the loaded_model for predictions\n",
    "loaded_y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename of the saved model\n",
    "model_filename = 'neural_network_model.pkl'\n",
    "\n",
    "# Load the saved model\n",
    "with open(model_filename, 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
